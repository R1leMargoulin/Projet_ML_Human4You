{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Théorie\n",
    "## Perceptron\n",
    "\n",
    "Voici un schéma de perceptron:\n",
    "\n",
    "<img src=\"./images/X.png\">\n",
    "\n",
    "Ici:\n",
    "- X représente nos entrées ($X_1$,$X_2$, ..., $X_n$)\n",
    "- Ŷ représente notre sortie, notre prédiction.\n",
    "- les $\\omega$ ($\\omega_1$,$\\omega_2$, ..., $\\omega_n$) représentent les poids a faire varier.\n",
    "\n",
    "Avec ces données, voici la fonction F(X) q'utilisera notre perceptron:\n",
    "\n",
    "$$ F(X) = ŷ = a \\begin{pmatrix}\\sum_{i=1}^n{(\\omega_ix_i)+\\omega_0}\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "a représente notre fonction d'activation.\n",
    "Etant en classification, notre fonction d'activation sera une fonction sigmoïde.\n",
    "\n",
    "Ainsi: \n",
    "$$ F(X) = ŷ = \\frac{1}{1+e^{\\begin{pmatrix}\\sum_{i=1}^n{(\\omega_ix_i)+\\omega_0}\\end{pmatrix}}} $$\n",
    "\n",
    "\n",
    "## Apprentissage\n",
    "Pour apprendre il nous faut donc faire varier notre $w$ au fil des itérations. Nous allons utiliser pour cela la descente de gradient:\n",
    "\n",
    "$$\\omega _i^{t+1} = \\omega_i^t-\\eta\\frac{\\delta erreur}{\\delta \\omega _i^t}\\\\\n",
    "\\ \\\\\n",
    "erreur(y_t,ŷ_t) = -y_t-log(ŷ_t)-(1--y_t-log(ŷ_t))\\\\\n",
    "\\ \\\\\n",
    "donc\\ \\omega _i^{t+1} = \\omega_i^t-\\eta\\frac{\\delta (-y_t-log(ŷ_t)-(1--y_t-log(ŷ_t)))}{\\delta \\omega _i^t}$$\n",
    "\n",
    "Pour l'erreur, on prend une entropie croisée étant en classification. On aurait pris une erreur quadratique pour de la regression.\n",
    "\n",
    "$\\eta$ représente notre learning rate, c'est la \"vitesse d'apprentissage\".\n",
    "\n",
    "Le but va être au fil des itération, de minimiser nos erreurs et de maximiser nos prédictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class Perceptron_classifier():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.w = np.array([])\n",
    "        self.X = np.array([])\n",
    "        self.y = np.array([])\n",
    "\n",
    "\n",
    "    def fit(self, X, y, num_epoch, batch_size):\n",
    "        n_inputs = len(X)\n",
    "        n_rows = len(X[0])\n",
    "        for i in range(n_inputs):\n",
    "            self.X = np.insert(self.X, 0, X[i], axis=0)\n",
    "            self.y = np.insert(self.X, 0, y[i], axis=0)\n",
    "            self.w = np.insert(self.w, 0, round(random.uniform(0.1, 1.1),1), axis=0)  #poids aleatoires au demarrage\n",
    "        \n",
    "        if(num_epoch * batch_size < n_rows):\n",
    "            #continue\n",
    "            for i in range(num_epoch):\n",
    "                error = []\n",
    "                for j in range(batch_size):\n",
    "                    yp = self.predict([X[num_epoch+batch_size, :]]) #x[0,:]= row0, toute les cols\n",
    "                    #erreur\n",
    "                #gradient descend pour evoluer les poids w\n",
    "\n",
    "        else:\n",
    "            print (\"invalid nums of epoch and/or batch size!!!\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        sum = 0\n",
    "    \n",
    "    def error(self):\n",
    "        a=0\n",
    "    def weight_update(self):\n",
    "        a=0\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
